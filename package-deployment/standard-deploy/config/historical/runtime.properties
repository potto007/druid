druid.service=historical

# Cache configs
#druid.historical.cache.useCache=true
#druid.historical.cache.populateCache=true

# Tier configs
#default
#druid.server.priority=0

# While we could set this here, it is desirable to use some math based on the number of brokers in launch scripts (e.g. numThreads=numBrokers*druid.broker.http.numConnections)
# So push these settings into the launch scripts.
# From Hagen Rother:
# The sum of all druid.broker.http.numConnections must be smaller than each druid.server.http.numThreads (i.e. check each historical and realtime)
# (If the number of threads is larger, one can end up with a deadlock situation distributing the queries to the realtime/historical query nodes.)
#druid.server.http.numThreads=100

#default
# Useful to make these configurable.  They also impact -XX:MaxDirectMemorySize since if get it wrong, get error like:
#   Please adjust -XX:MaxDirectMemorySize, druid.processing.buffer.sizeBytes, or druid.processing.numThreads: maxDirectMemory[12,240,683,008], memoryNeeded[18,253,611,008] = druid.processing.buffer.sizeBytes[1,073,741,824] * ( druid.processing.numThreads[16] + 1 )
#  So set it in a calculation.
#druid.processing.buffer.sizeBytes=1073741824
#druid.processing.numThreads=16
#druid.server.maxSize=5497559962838
#druid.segmentCache.locations=[{"path": "/druid/local/historical/indexCache", "maxSize"\: 5497559962838}]

com.metamx.query.groupBy.maxResults=5000000
druid.query.groupBy.maxResults=5000000